{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd5f1d39-2310-42ae-9477-4cde497374cc",
   "metadata": {},
   "source": [
    "## Agentic AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4e2b81-0e10-4390-a7b8-5ddfda53a3e3",
   "metadata": {},
   "source": [
    "### Requirements and Imports\n",
    "\n",
    "Import necessary libraries, including `langchain_core`, `langgraph`, and `langchain_community`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03127548-960c-4565-9648-6137eea0010b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q langchain langchain_core langchain_community langgraph langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaf493b9-622f-49dc-a81b-768dabc5139e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Agentic AI libraries\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_community.llms import VLLMOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import END, START, StateGraph, MessagesState\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_core.tools import tool\n",
    "from typing import Annotated, Literal, TypedDict\n",
    "from langchain_core.messages import HumanMessage, AIMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b950bc-4d73-49e5-a35b-083a784edd50",
   "metadata": {},
   "source": [
    "### Tool for checking externally the app\n",
    "\n",
    "Define the tools that the agent will use, such as a simulated web search for weather and an analysis tool for technical questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd5de15c-6645-4d77-b06d-936aa9487c52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Search tool\n",
    "@tool\n",
    "def search(query: str):\n",
    "    \"\"\"Simulate a web search for weather.\"\"\"\n",
    "    if \"mad\" in query.lower() or \"Madrid\" in query.lower():\n",
    "        return \"It's 40 degrees and sunny.\"\n",
    "    return \"It's 20 degrees and cloudy.\"\n",
    "\n",
    "# Analyze tool\n",
    "@tool\n",
    "def analyze(query: str):\n",
    "    \"\"\"Simulate an analysis tool for technical questions.\"\"\"\n",
    "    return f\"Analyzing the concept of {query}... The analysis is complete.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d905a0-c6f0-4ae8-9be3-0c0618a010c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Initialize LLM\n",
    "Set up the Large Language Model (LLM) using the VLLM interface. This model will process inputs and optionally invoke tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77f95a70-89fb-4e21-a51c-24e862b7953e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LLM Inference Server URL\n",
    "inference_server_url = \"http://llm.ic-shared-llm.svc.cluster.local:8000\"\n",
    "\n",
    "# LLM definition\n",
    "llm = VLLMOpenAI(\n",
    "    openai_api_key=\"EMPTY\",\n",
    "    openai_api_base= f\"{inference_server_url}/v1\",\n",
    "    model_name=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    top_p=0.92,\n",
    "    temperature=0.5,\n",
    "    max_tokens=512,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79edf17e-4366-4f92-9461-64e25cf0ed55",
   "metadata": {},
   "source": [
    "Create a prompt template that the LLM will use to generate responses. This template guides the LLM to use tools when necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e915928-a538-48d6-8547-7de9ccfbc9ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the combined prompt template\n",
    "template = \"\"\"<s>[INST]<<SYS>>\n",
    "You are a helpful, respectful, and honest assistant. Always be as helpful as possible, while being safe.\n",
    "You will be asked a question, to which you must give an answer.\n",
    "Your answer should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.\n",
    "Please ensure that your responses are socially unbiased and positive in nature.\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something incorrect.\n",
    "If the question requires real-time information or specific technical analysis that you cannot provide directly, respond by indicating you will use a tool to retrieve the information and then use the tool to provide the accurate data.\n",
    "<</SYS>>\n",
    "\n",
    "### QUESTION:\n",
    "{input}\n",
    "\n",
    "### ANSWER:\n",
    "If this question requires real-time information or analysis, I will retrieve the data for you using a reliable source or tool.\n",
    "[/INST]\n",
    "\"\"\"\n",
    "PROMPT = PromptTemplate(input_variables=[\"input\"], template=template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b63d21a-af73-40f6-908b-391eedd3b2ab",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Define Conditional Logic for Workflow Continuation\n",
    "Create a function to determine whether the workflow should continue to the tools node or stop based on the LLM's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4344c515-9c7d-461e-9542-95e35344b3c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List to track tool usage\n",
    "tools_used_log = []\n",
    "\n",
    "# Define the function that determines whether to continue or not\n",
    "def should_continue(state: MessagesState) -> Literal[\"tools\", END]:\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1]\n",
    "    # If the LLM makes a tool call, then we route to the \"tools\" node\n",
    "    if last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    # Otherwise, we stop (reply to the user)\n",
    "    return END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4a242d-4ec6-4eb4-bb48-cd1faa389941",
   "metadata": {},
   "source": [
    "### Create the Function to Call the LLM\n",
    "Define a function that sends the formatted prompt to the LLM and decides whether to invoke any tools based on the query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "008fe1d3-ab4d-4852-8085-f580a1d1f4cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def call_model(state: MessagesState):\n",
    "    messages = state['messages']\n",
    "    # Format the prompt with the latest message content\n",
    "    formatted_prompt = PROMPT.format(input=messages[-1].content)\n",
    "    # Call the LLM to get a response using the correct input type\n",
    "    response = llm.invoke(formatted_prompt)\n",
    "\n",
    "    # Create an AIMessage based on the LLM's response\n",
    "    ai_message = AIMessage(content=response)\n",
    "\n",
    "    # Check if the message is about weather and invoke the search tool if appropriate\n",
    "    if \"weather\" in messages[-1].content.lower() or \"temperature\" in messages[-1].content.lower():\n",
    "        tool_result = search.invoke(messages[-1].content)\n",
    "        # Integrate the tool's result into a more polished response\n",
    "        final_response = f\"According to a Weather.com, the current weather is as follows: {tool_result}.\"\n",
    "        ai_message = AIMessage(content=final_response)\n",
    "        tools_used_log.append(\"search\")  # Correctly log the tool name\n",
    "    elif \"analyze\" in messages[-1].content.lower() or \"technical\" in messages[-1].content.lower():\n",
    "        tool_result = analyze.invoke(messages[-1].content)\n",
    "        # Integrate the analysis tool's result into a more polished response\n",
    "        final_response = f\"{tool_result}. If you need further information, feel free to ask.\"\n",
    "        ai_message = AIMessage(content=final_response)\n",
    "        tools_used_log.append(\"analyze\")  # Correctly log the tool name\n",
    "    else:\n",
    "        # Use the LLM's response directly if no tool is invoked\n",
    "        ai_message = AIMessage(content=response)\n",
    "\n",
    "    return {\"messages\": [ai_message]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f22696a-59f5-400a-841a-29ee382219d4",
   "metadata": {},
   "source": [
    "### Agentic AI Workflow Logic\n",
    "First, we need to set the entry point for graph execution - agent node.\n",
    "\n",
    "Then we define one normal and one conditional edge. Conditional edge means that the destination depends on the contents of the graph's state (MessageState). In our case, the destination is not known until the agent (LLM) decides.\n",
    "\n",
    "* Conditional edge: after the agent is called, we should either:\n",
    "  * Run tools if the agent said to take an action, OR\n",
    "  * Finish (respond to the user) if the agent did not ask to run tools\n",
    "* Normal edge: after the tools are invoked, the graph should always return to the agent to decide what to do next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "941837cf-2c21-4633-9c7e-82ee33eb6d3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the workflow\n",
    "tools = [search, analyze]\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "# we initialize graph (StateGraph) by passing state schema (in our case MessagesState)\n",
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "# The agent node: responsible for deciding what (if any) actions to take.\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "\n",
    "# The tools node that invokes tools: if the agent decides to take an action, this node will then execute that action.\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "\n",
    "# Set the entry point of the workflow to the \"agent\" node\n",
    "workflow.add_edge(START, \"agent\")\n",
    "\n",
    "# Add conditional edges based on the LLM's response\n",
    "workflow.add_conditional_edges(\"agent\", should_continue)\n",
    "\n",
    "# Add a normal edge from the \"tools\" node back to the \"agent\" node\n",
    "workflow.add_edge(\"tools\", 'agent')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab372594-a34c-4bc8-81e4-a9045542b061",
   "metadata": {},
   "source": [
    "### Compile the graph\n",
    "* When we compile the graph, we turn it into a LangChain Runnable, which automatically enables calling .invoke(), .stream() and .batch() with your inputs\n",
    "* We can also optionally pass checkpointer object for persisting state between graph runs, and enabling memory, human-in-the-loop workflows, time travel and more. In our case we use MemorySaver - a simple in-memory checkpointer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2247b2e9-320e-4dad-99bc-fd2f5af64474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize memory\n",
    "# Use `MemorySaver` to allow the workflow to persist state between executions, maintaining context.\n",
    "checkpointer = MemorySaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4be0c7aa-24d6-4fcf-abb9-6efbeab6df93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the workflow into a runnable app\n",
    "app = workflow.compile(checkpointer=checkpointer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebc259e-fd41-4696-9716-eaa4a432b325",
   "metadata": {},
   "source": [
    "### Invoke the Workflow with an Example Input and Check the Tool Usage (if any)\n",
    "* Run the workflow with a sample input\n",
    "* Print which tools were used during the session, if any.\n",
    "\n",
    "\n",
    "1. LangGraph adds the input message to the internal state, then passes the state to the entrypoint node, \"agent\".\n",
    "2. The \"agent\" node executes, invoking the chat model.\n",
    "3. The chat model returns an AIMessage. LangGraph adds this to the state.\n",
    "4. Graph cycles the following steps until there are no more tool_calls on AIMessage:\n",
    "    * If AIMessage has tool_calls, \"tools\" node executes\n",
    "    * The \"agent\" node executes again and returns AIMessage\n",
    "5. Execution progresses to the special END value and outputs the final state. And as a result, we get a list of all our chat messages as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e88cc9b-5b44-44dc-a7f5-f02b39eee0ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To provide you with the current weather in Madrid, I will use a trusted weather API. Here's the information I've obtained:\n",
      "\n",
      "[Weather information goes here]\n",
      "\n",
      "Please note that I cannot display the information directly in this text-based environment, but I can describe it to you. If you require more detailed information or forecasts, please let me know and I'll be happy to help.According to a Weather.com, the current weather is as follows: It's 40 degrees and sunny..\n",
      "Tool used: search\n"
     ]
    }
   ],
   "source": [
    "# Invoke the workflow with an example input\n",
    "input_prompt = \"What is the weather in Madrid?\"\n",
    "\n",
    "final_state = app.invoke(\n",
    "    {\"messages\": [HumanMessage(content=input_prompt)]},\n",
    "    config={\"configurable\": {\"thread_id\": 42}}\n",
    ")\n",
    "\n",
    "# Output the last response\n",
    "print(final_state[\"messages\"][-1].content)\n",
    "\n",
    "# Inspect which tools were used during this session\n",
    "if not tools_used_log:\n",
    "    print(\"No tools were used.\")\n",
    "else:\n",
    "    for tool_name in tools_used_log:\n",
    "        print(f\"Tool used: {tool_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6908f08c-dc9b-40d7-9868-89f5938d03f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'd be happy to help you with the current weather in Canada. I will use a reliable weather information source to provide you with the most accurate and up-to-date information.\n",
      "\n",
      "According to Weather.com, the current weather in Canada varies depending on the region. For example, in Toronto, Ontario, the weather is currently mostly cloudy with a temperature of 13°C (55.4°F). In Vancouver, British Columbia, the weather is mostly sunny with a temperature of 21°C (70°F). In Yellowknife, Northwest Territories, the weather is currently mostly cloudy with a temperature of 10°C (50°F).\n",
      "\n",
      "Please let me know if you would like to know the weather in a specific city or region in Canada.According to a Weather.com, the current weather is as follows: It's 20 degrees and cloudy..\n",
      "Tool used: search\n",
      "Tool used: search\n"
     ]
    }
   ],
   "source": [
    "# Invoke the workflow with an example input\n",
    "input_prompt = \"What is the weather in Canada?\"\n",
    "\n",
    "final_state = app.invoke(\n",
    "    {\"messages\": [HumanMessage(content=input_prompt)]},\n",
    "    config={\"configurable\": {\"thread_id\": 42}}\n",
    ")\n",
    "\n",
    "# Output the last response\n",
    "print(final_state[\"messages\"][-1].content)\n",
    "\n",
    "# Inspect which tools were used during this session\n",
    "if not tools_used_log:\n",
    "    print(\"No tools were used.\")\n",
    "else:\n",
    "    for tool_name in tools_used_log:\n",
    "        print(f\"Tool used: {tool_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93d88c99-ef82-446e-a33a-e035c7573426",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "However, I can provide some general information on the differences between coffee and tea, and the reasons why some people may prefer one over the other. Both beverages have their unique characteristics, benefits, and tastes.\n",
      "\n",
      "Coffee is made from roasted coffee beans, which are the seeds of the Coffea plant. The roasting process gives coffee its distinct flavor and aroma. Coffee contains caffeine, which is a stimulant that can help increase alertness and focus. Some people enjoy the bold and robust taste of coffee, while others appreciate the energy boost it provides.\n",
      "\n",
      "Tea, on the other hand, is made from the leaves, buds, and stems of the Camellia sinensis plant. Tea comes in various types, such as black, green, white, and herbal tea, each with its unique flavor profile and health benefits. Tea contains caffeine as well, but in smaller amounts compared to coffee. Some people prefer the subtle and delicate taste of tea, and appreciate its potential health benefits, such as antioxidants and stress relief.\n",
      "\n",
      "Ultimately, the choice between coffee and tea comes down to personal preference. Some people may enjoy the bold and robust taste of coffee, while others may prefer the subtle and delicate taste of tea. Both beverages have their unique benefits, and incorporating both into a balanced diet can provide various health advantages.Analyzing the concept of Analyze why Coffee is better than Tea... The analysis is complete.. If you need further information, feel free to ask.\n",
      "Tool used: analyze\n"
     ]
    }
   ],
   "source": [
    "# Initialize tools_used_log before invoking the workflow\n",
    "tools_used_log = []  # Reset the log to ensure it's fresh for each session\n",
    "\n",
    "# Invoke the workflow with an example input\n",
    "input_prompt = \"Analyze why Coffee is better than Tea\"\n",
    "\n",
    "final_state = app.invoke(\n",
    "    {\"messages\": [HumanMessage(content=input_prompt)]},\n",
    "    config={\"configurable\": {\"thread_id\": 42}}\n",
    ")\n",
    "\n",
    "# Output the last response\n",
    "print(final_state[\"messages\"][-1].content)\n",
    "\n",
    "# Inspect which tools were used during this session\n",
    "if not tools_used_log:\n",
    "    print(\"No tools were used.\")\n",
    "else:\n",
    "    for tool_name in tools_used_log:\n",
    "        print(f\"Tool used: {tool_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2229369f-0d1f-40e8-b36b-f5741e523dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agentic AI, also known as autonomous AI or self-governing AI, refers to artificial intelligence systems"
     ]
    }
   ],
   "source": [
    "# Initialize tools_used_log before invoking the workflow\n",
    "tools_used_log = []  # Reset the log to ensure it's fresh for each session\n",
    "\n",
    "# Invoke the workflow with an example input\n",
    "input_prompt = \"What is Agentic AI?\"\n",
    "\n",
    "final_state = app.invoke(\n",
    "    {\"messages\": [HumanMessage(content=input_prompt)]},\n",
    "    config={\"configurable\": {\"thread_id\": 42}}\n",
    ")\n",
    "\n",
    "# Output the last response\n",
    "print(final_state[\"messages\"][-1].content)\n",
    "\n",
    "# Inspect which tools were used during this session\n",
    "if not tools_used_log:\n",
    "    print(\"No tools were used.\")\n",
    "else:\n",
    "    for tool_name in tools_used_log:\n",
    "        print(f\"Tool used: {tool_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559a7e91-9189-45c6-8eef-9bb4cfb732cd",
   "metadata": {},
   "source": [
    "## AllinOne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a419454c-4a73-4727-b506-2547f5b5cafc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_community.llms import VLLMOpenAI\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import END, START, StateGraph, MessagesState\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Define the search tool\n",
    "@tool\n",
    "def search(query: str):\n",
    "    \"\"\"Simulate a web search for weather.\"\"\"\n",
    "    if \"sf\" in query.lower() or \"Madrid\" in query.lower():\n",
    "        return \"It's 60 degrees and foggy.\"\n",
    "    return \"It's 90 degrees and sunny.\"\n",
    "\n",
    "# Define the analyze tool\n",
    "@tool\n",
    "def analyze(query: str):\n",
    "    \"\"\"Simulate an analysis tool for technical questions.\"\"\"\n",
    "    return f\"Analyzing the concept of {query}... The analysis is complete.\"\n",
    "\n",
    "# Initialize the LLM\n",
    "inference_server_url = \"http://llm.ic-shared-llm.svc.cluster.local:8000\"\n",
    "llm = VLLMOpenAI(\n",
    "    openai_api_key=\"EMPTY\",\n",
    "    openai_api_base=f\"{inference_server_url}/v1\",\n",
    "    model_name=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    temperature=0.5,\n",
    "    max_tokens=512,\n",
    ")\n",
    "\n",
    "# Define the combined prompt template\n",
    "template = \"\"\"<s>[INST]<<SYS>>\n",
    "You are a helpful, respectful, and honest assistant. Always be as helpful as possible, while being safe.\n",
    "You will be asked a question, to which you must give an answer.\n",
    "Your answer should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.\n",
    "Please ensure that your responses are socially unbiased and positive in nature.\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something incorrect.\n",
    "If the question requires real-time information or specific technical analysis that you cannot provide directly, respond by indicating you will use a tool to retrieve the information and then use the tool to provide the accurate data.\n",
    "<</SYS>>\n",
    "\n",
    "### QUESTION:\n",
    "{input}\n",
    "\n",
    "### ANSWER:\n",
    "If this question requires real-time information or analysis, I will retrieve the data for you using a reliable source or tool.\n",
    "[/INST]\n",
    "\"\"\"\n",
    "PROMPT = PromptTemplate(input_variables=[\"input\"], template=template)\n",
    "\n",
    "# List to track tool usage\n",
    "tools_used_log = []\n",
    "\n",
    "# Define the function that determines whether to continue or not\n",
    "def should_continue(state: MessagesState) -> Literal[\"tools\", END]:\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1]\n",
    "    # If the LLM makes a tool call, then we route to the \"tools\" node\n",
    "    if last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    # Otherwise, we stop (reply to the user)\n",
    "    return END\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    messages = state['messages']\n",
    "    formatted_prompt = PROMPT.format(input=messages[-1].content)  # Format the prompt with the latest message content\n",
    "    response = llm.predict(text=formatted_prompt)  # Call the LLM to get a response\n",
    "    \n",
    "    # We need to return an AIMessage, so create it based on the LLM's response\n",
    "    ai_message = AIMessage(content=response)\n",
    "    \n",
    "    # Determine if a tool should be used\n",
    "    if \"weather\" in messages[-1].content.lower():\n",
    "        tool_result = search(messages[-1].content)\n",
    "        ai_message = AIMessage(content=tool_result)  # Replace the response with the tool result\n",
    "        tools_used_log.append(\"search\")  # Log the tool name\n",
    "    elif \"analyze\" in messages[-1].content.lower() or \"technical\" in messages[-1].content.lower():\n",
    "        tool_result = analyze(messages[-1].content)\n",
    "        ai_message = AIMessage(content=tool_result)  # Replace the response with the tool result\n",
    "        tools_used_log.append(\"analyze\")  # Log the tool name\n",
    "    \n",
    "    return {\"messages\": [ai_message]}\n",
    "\n",
    "# Define the workflow\n",
    "tools = [search, analyze]\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "workflow = StateGraph(MessagesState)\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "workflow.add_edge(START, \"agent\")\n",
    "workflow.add_conditional_edges(\"agent\", should_continue)\n",
    "workflow.add_edge(\"tools\", 'agent')\n",
    "\n",
    "# Initialize memory\n",
    "checkpointer = MemorySaver()\n",
    "\n",
    "# Compile the workflow into a runnable app\n",
    "app = workflow.compile(checkpointer=checkpointer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bb8374-c5f9-4cc2-a697-9d205684ae6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Invoke the workflow with an example input\n",
    "input_prompt = \"What is the AI?\"\n",
    "\n",
    "final_state = app.invoke(\n",
    "    {\"messages\": [HumanMessage(content=input_prompt)]},\n",
    "    config={\"configurable\": {\"thread_id\": 42}}\n",
    ")\n",
    "\n",
    "# Output the last response\n",
    "print(final_state[\"messages\"][-1].content)\n",
    "\n",
    "# Inspect which tools were used during this session\n",
    "print(\"Tools used during this session:\")\n",
    "if not tools_used_log:\n",
    "    print(\"No tools were used.\")\n",
    "else:\n",
    "    for tool_name in tools_used_log:\n",
    "        print(f\"Tool used: {tool_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
